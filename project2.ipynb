{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CITS4012 project 2\n",
    "\n",
    "Authors: Theoridho Andily [22764884] & "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "job_description = pd.read_csv('seek_australia.csv', usecols=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need more data cleaning to remove non alphanum characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = job_description.job_description[:2000].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokenized = gensim.utils.simple_preprocess(job_description, deacc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = Dictionary()\n",
    "# BoW_corpus = dictionary.doc2bow(doc_tokenized, allow_update=True)\n",
    "# BoW_corpus = [(dictionary[id], freq) for id, freq in BoW_corpus]\n",
    "# BoW_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert(tup, dict):\n",
    "#     for a, b in tup:\n",
    "#         dict[a] = b\n",
    "#     return dict\n",
    "# BoW_corpus_dict = dict()\n",
    "# BoW_dict = convert(BoW_corpus, BoW_corpus_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = 16\n",
    "model = Word2Vec(min_count=1,\n",
    "                     window=2,\n",
    "                     vector_size=100,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t = time()\n",
    "\n",
    "model.build_vocab([doc_tokenized], progress_per=10)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [48]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m t \u001B[38;5;241m=\u001B[39m time()\n\u001B[1;32m----> 3\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc_tokenized\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorpus_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreport_delay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTime to train the model: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m mins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mround\u001B[39m((time() \u001B[38;5;241m-\u001B[39m t) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m, \u001B[38;5;241m2\u001B[39m)))\n",
      "File \u001B[1;32mc:\\envs\\cits4012\\lib\\site-packages\\gensim\\models\\word2vec.py:1069\u001B[0m, in \u001B[0;36mWord2Vec.train\u001B[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m   1066\u001B[0m     callback\u001B[38;5;241m.\u001B[39mon_epoch_begin(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1068\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1069\u001B[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1070\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcur_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcur_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_examples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1071\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_words\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqueue_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqueue_factor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreport_delay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreport_delay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1072\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1073\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1074\u001B[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_epoch_corpusfile(\n\u001B[0;32m   1075\u001B[0m         corpus_file, cur_epoch\u001B[38;5;241m=\u001B[39mcur_epoch, total_examples\u001B[38;5;241m=\u001B[39mtotal_examples, total_words\u001B[38;5;241m=\u001B[39mtotal_words,\n\u001B[0;32m   1076\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\envs\\cits4012\\lib\\site-packages\\gensim\\models\\word2vec.py:1430\u001B[0m, in \u001B[0;36mWord2Vec._train_epoch\u001B[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001B[0m\n\u001B[0;32m   1427\u001B[0m     thread\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# make interrupting the process with ctrl+c easier\u001B[39;00m\n\u001B[0;32m   1428\u001B[0m     thread\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m-> 1430\u001B[0m trained_word_count, raw_word_count, job_tally \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_epoch_progress\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1431\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprogress_queue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjob_queue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcur_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcur_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_examples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1432\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtotal_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_words\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreport_delay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreport_delay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_corpus_file_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1433\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1435\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001B[1;32mc:\\envs\\cits4012\\lib\\site-packages\\gensim\\models\\word2vec.py:1285\u001B[0m, in \u001B[0;36mWord2Vec._log_epoch_progress\u001B[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001B[0m\n\u001B[0;32m   1282\u001B[0m unfinished_worker_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworkers\n\u001B[0;32m   1284\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m unfinished_worker_count \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 1285\u001B[0m     report \u001B[38;5;241m=\u001B[39m \u001B[43mprogress_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# blocks if workers too slow\u001B[39;00m\n\u001B[0;32m   1286\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m report \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# a thread reporting that it finished\u001B[39;00m\n\u001B[0;32m   1287\u001B[0m         unfinished_worker_count \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mc:\\envs\\cits4012\\lib\\queue.py:170\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_qsize():\n\u001B[1;32m--> 170\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnot_empty\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m timeout \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m must be a non-negative number\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mc:\\envs\\cits4012\\lib\\threading.py:302\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    300\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[0;32m    301\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 302\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    303\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "model.train(doc_tokenized, total_examples=model.corpus_count, epochs=1000, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [37]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#print(vocab_sorted)\u001B[39;00m\n\u001B[0;32m      7\u001B[0m stopwords \u001B[38;5;241m=\u001B[39m stopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m input_vocab \u001B[38;5;241m=\u001B[39m  [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m vocab_sorted \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m wv\u001B[38;5;241m.\u001B[39mkey_to_index\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;129;01mand\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stopwords]\n\u001B[0;32m      9\u001B[0m points \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(input_vocab)\n\u001B[0;32m     10\u001B[0m X \u001B[38;5;241m=\u001B[39m wv[input_vocab]\n",
      "Input \u001B[1;32mIn [37]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#print(vocab_sorted)\u001B[39;00m\n\u001B[0;32m      7\u001B[0m stopwords \u001B[38;5;241m=\u001B[39m stopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m input_vocab \u001B[38;5;241m=\u001B[39m  [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m vocab_sorted \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[43mwv\u001B[49m\u001B[38;5;241m.\u001B[39mkey_to_index\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;129;01mand\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stopwords]\n\u001B[0;32m      9\u001B[0m points \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(input_vocab)\n\u001B[0;32m     10\u001B[0m X \u001B[38;5;241m=\u001B[39m wv[input_vocab]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'wv' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vocab_sorted = dict(sorted(BoW_corpus_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "#print(vocab_sorted)\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "input_vocab =  [word for word in vocab_sorted if word in wv.key_to_index.keys() and word not in stopwords]\n",
    "points = len(input_vocab)\n",
    "X = wv[input_vocab]\n",
    "X_tsne = tsne.fit_transform(X[:points])\n",
    "\n",
    "#print(input_vocab)\n",
    "\n",
    "#points = len(input_vocab)\n",
    "\n",
    "interactive_tsne(list(input_vocab)[:points], X_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}